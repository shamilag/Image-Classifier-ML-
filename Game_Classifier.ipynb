{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06616fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e34ffc",
   "metadata": {},
   "source": [
    "# Import-Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "656eb6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a6524",
   "metadata": {},
   "source": [
    "# Config class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "764003e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    BASE_PATH = \"C:/Users/Shamila/OneDrive/Desktop/CI_Assign\"\n",
    "    IMG_SIZE = 224\n",
    "    RANDOM_STATE = 42\n",
    "    TEST_SIZE = 0.2\n",
    "    VAL_SIZE = 0.15\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 30\n",
    "    NUM_CLASSES = None \n",
    "    \n",
    "    # Visualization settings\n",
    "    PLOT_STYLE = 'ggplot'\n",
    "    FIG_SIZE = (10, 6)\n",
    "    CMAP = 'YlGnBu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e3eb8",
   "metadata": {},
   "source": [
    "# Set-matplotlib-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "285bc7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(Config.PLOT_STYLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba795e3a",
   "metadata": {},
   "source": [
    "# Data-Loader-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e08bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        self.train_df = pd.read_csv(os.path.join(Config.BASE_PATH, \"train.csv\"))\n",
    "        self.test_df = pd.read_csv(os.path.join(Config.BASE_PATH, \"test.csv\"))\n",
    "        self.le = LabelEncoder()\n",
    "        self.class_names = None\n",
    "\n",
    "    def _analyze_images(self, df):\n",
    "        \"dimensions and aspect ratios\"\n",
    "        widths = []\n",
    "        heights = []\n",
    "        aspect_ratios = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            img_path = os.path.join(Config.BASE_PATH, row['filename'])\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    width, height = img.size\n",
    "                    widths.append(width)\n",
    "                    heights.append(height)\n",
    "                    aspect_ratios.append(width / height)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        return widths, heights, aspect_ratios\n",
    "\n",
    "    def plot_image_stats(self):\n",
    "        \"statistics visualizations\"\n",
    "        widths, heights, aspect_ratios = self._analyze_images(self.train_df)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "        sns.histplot(widths, bins=30, color='mediumseagreen', ax=axes[0, 0], kde=True)\n",
    "        axes[0, 0].set_title('Image Width Distribution')\n",
    "        axes[0, 0].set_xlabel('Width (pixels)')\n",
    "        axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "        sns.histplot(heights, bins=30, color='seagreen', ax=axes[0, 1], kde=True)\n",
    "        axes[0, 1].set_title('Image Height Distribution')\n",
    "        axes[0, 1].set_xlabel('Height (pixels)')\n",
    "        axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "        sns.histplot(aspect_ratios, bins=30, color='palegreen', ax=axes[1, 0], kde=True)\n",
    "        axes[1, 0].set_title('Aspect Ratio Distribution')\n",
    "        axes[1, 0].set_xlabel('Aspect Ratio (width/height)')\n",
    "        axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "        sns.scatterplot(x=widths, y=heights, alpha=0.6, color='forestgreen', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Width vs Height Scatter Plot')\n",
    "        axes[1, 1].set_xlabel('Width (pixels)')\n",
    "        axes[1, 1].set_ylabel('Height (pixels)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('statistics.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Boxplot-aspect-ratios\n",
    "        plt.figure(figsize=Config.FIG_SIZE)\n",
    "        sns.boxplot(x=aspect_ratios, color='mediumseagreen')\n",
    "        plt.title('Aspect Ratio Boxplot')\n",
    "        plt.xlabel('Aspect Ratio')\n",
    "        plt.savefig('aspectratioboxplot.png')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_class_distribution(self):\n",
    "        \"Plot class distribution as pie chart and bar graph\"\n",
    "        class_counts = self.train_df['class'].value_counts()\n",
    "        self.class_names = class_counts.index.tolist()\n",
    "        Config.NUM_CLASSES = len(self.class_names)\n",
    "\n",
    "        # Pie-chart\n",
    "        plt.figure(figsize=Config.FIG_SIZE)\n",
    "        plt.pie(class_counts, labels=self.class_names, autopct='%1.1f%%',\n",
    "                startangle=90, colors=sns.color_palette('Greens', len(class_counts)))\n",
    "        plt.title('Class Distribution (Pie Chart)')\n",
    "        plt.savefig('piechart.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Bar-graph\n",
    "        plt.figure(figsize=Config.FIG_SIZE)\n",
    "        sns.barplot(x=class_counts.index, y=class_counts.values, palette='Greens')\n",
    "        plt.title('Class Distribution (Bar Graph)')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('distribution-bar.png')\n",
    "        plt.close()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"Load and preprocess images\"\n",
    "        def _process_df(df, label=True):\n",
    "            X, y = [], []\n",
    "            for _, row in df.iterrows():\n",
    "                img = cv2.imread(os.path.join(Config.BASE_PATH, row['filename']))\n",
    "                if img is None:\n",
    "                    continue\n",
    "\n",
    "                img_resized = cv2.resize(img, (Config.IMG_SIZE, Config.IMG_SIZE))\n",
    "                X.append(preprocess_input(img_resized))\n",
    "\n",
    "                if label:\n",
    "                    y.append(row['class'])\n",
    "\n",
    "            return (np.array(X), np.array(y)) if label else np.array(X)\n",
    "\n",
    "        # Plot-datastatistics\n",
    "        self.plot_image_stats()\n",
    "        self.plot_class_distribution()\n",
    "\n",
    "        # Load-trainingdata\n",
    "        X_train, y_train = _process_df(self.train_df, label=True)\n",
    "        X_test = _process_df(self.test_df, label=False)\n",
    "\n",
    "        # Encode-labels\n",
    "        y_encoded = self.le.fit_transform(y_train)\n",
    "\n",
    "        return {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_encoded,\n",
    "            'X_test': X_test,\n",
    "            'class_names': self.class_names\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7184eb",
   "metadata": {},
   "source": [
    "# Neural-Network-Model-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbbf2772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkModel:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.model = self.build_model()\n",
    "        self.history = None\n",
    "    \n",
    "    def build_model(self):\n",
    "        \" transfer learning used Build to EfficientNetB0 based model\"\n",
    "        base_model = EfficientNetB0(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(Config.IMG_SIZE, Config.IMG_SIZE, 3)\n",
    "        )\n",
    "        \n",
    "        #  Fine-tune-later-ones and Freeze-initial-layers\n",
    "        for layer in base_model.layers[:100]:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[100:]:\n",
    "            layer.trainable = True\n",
    "        \n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        outputs = Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = Model(inputs=base_model.input, outputs=outputs)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_data_augmenter(self):\n",
    "        \" augmentation used to create image data generator \"\n",
    "        return ImageDataGenerator(\n",
    "            rotation_range=30,\n",
    "            zoom_range=0.2,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            brightness_range=[0.8, 1.2],\n",
    "            shear_range=0.1,\n",
    "            fill_mode='nearest',\n",
    "            validation_split=Config.VAL_SIZE\n",
    "        )\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        \"data augmentation and callbacks used to train the model \"\n",
    "        datagen = self.get_data_augmenter()\n",
    "        \n",
    "        # Create - callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1),\n",
    "            ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            datagen.flow(X_train, y_train, batch_size=Config.BATCH_SIZE, subset='training'),\n",
    "            steps_per_epoch=int(len(X_train) * (1 - Config.VAL_SIZE) // Config.BATCH_SIZE),\n",
    "            \n",
    "            epochs=Config.EPOCHS,\n",
    "            validation_data=datagen.flow(X_train, y_train, batch_size=Config.BATCH_SIZE, subset='validation'),\n",
    "            validation_steps=int(len(X_train) * Config.VAL_SIZE // Config.BATCH_SIZE),\n",
    "            \n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "                )\n",
    "\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"Plot training & validation accuracy-loss\"\n",
    "        if self.history is None:\n",
    "            print(\"Model hasn't been trained yet!\")\n",
    "            return\n",
    "        \n",
    "        history = self.history.history\n",
    "        \n",
    "        plt.figure(figsize=(14, 5))\n",
    "        \n",
    "        # Plot-accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot-loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('traininghistory.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def evaluate(self, X_val, y_val, class_names):\n",
    "        \"Evaluate model & plot confusion matrix\"\n",
    "        y_pred = np.argmax(self.model.predict(X_val), axis=1)\n",
    "        \n",
    "        # Classification-report\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_val, y_pred, target_names=class_names))\n",
    "        \n",
    "        # Confusion-matrix\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=Config.CMAP, \n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusionmatrix.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b652f98",
   "metadata": {},
   "source": [
    "# Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7ed6252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shamila\\AppData\\Local\\Temp\\ipykernel_4664\\1244372516.py:81: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=class_counts.index, y=class_counts.values, palette='Greens')\n",
      "c:\\Users\\Shamila\\.conda\\envs\\shamila\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.2606 - loss: 2.5792"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 4s/step - accuracy: 0.2641 - loss: 2.5592 - val_accuracy: 0.4583 - val_loss: 1.4115 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m 1/21\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.1250 - loss: 3.3527"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shamila\\.conda\\envs\\shamila\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 398ms/step - accuracy: 0.1250 - loss: 3.3527 - val_accuracy: 0.5000 - val_loss: 1.4015 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.5514 - loss: 1.2395"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 4s/step - accuracy: 0.5538 - loss: 1.2323 - val_accuracy: 0.7396 - val_loss: 1.0739 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 224ms/step - accuracy: 0.6875 - loss: 0.8045 - val_accuracy: 0.7292 - val_loss: 1.0273 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7732 - loss: 0.5913"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 4s/step - accuracy: 0.7747 - loss: 0.5884 - val_accuracy: 0.9271 - val_loss: 0.6917 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 327ms/step - accuracy: 0.8750 - loss: 0.3264 - val_accuracy: 0.8542 - val_loss: 0.7360 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8838 - loss: 0.3114"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - accuracy: 0.8844 - loss: 0.3107 - val_accuracy: 0.9375 - val_loss: 0.4727 - learning_rate: 1.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 201ms/step - accuracy: 0.9062 - loss: 0.3581 - val_accuracy: 0.9271 - val_loss: 0.4836 - learning_rate: 1.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 3s/step - accuracy: 0.9116 - loss: 0.2377 - val_accuracy: 0.9375 - val_loss: 0.3409 - learning_rate: 1.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m 1/21\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 642ms/step - accuracy: 0.8750 - loss: 0.5101"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 245ms/step - accuracy: 0.8750 - loss: 0.5101 - val_accuracy: 0.9583 - val_loss: 0.2670 - learning_rate: 1.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9334 - loss: 0.1852"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 4s/step - accuracy: 0.9341 - loss: 0.1842 - val_accuracy: 0.9688 - val_loss: 0.2146 - learning_rate: 1.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m 1/21\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 4s/step - accuracy: 0.9375 - loss: 0.1418"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 443ms/step - accuracy: 0.9375 - loss: 0.1418 - val_accuracy: 0.9896 - val_loss: 0.1822 - learning_rate: 1.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 4s/step - accuracy: 0.9483 - loss: 0.1528 - val_accuracy: 0.9688 - val_loss: 0.1465 - learning_rate: 1.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 707ms/step - accuracy: 0.9688 - loss: 0.1909 - val_accuracy: 0.9896 - val_loss: 0.1324 - learning_rate: 1.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9644 - loss: 0.1277"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 4s/step - accuracy: 0.9643 - loss: 0.1278 - val_accuracy: 1.0000 - val_loss: 0.0824 - learning_rate: 1.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 278ms/step - accuracy: 0.9375 - loss: 0.1677 - val_accuracy: 0.9896 - val_loss: 0.0835 - learning_rate: 1.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 3s/step - accuracy: 0.9680 - loss: 0.1092 - val_accuracy: 0.9896 - val_loss: 0.0570 - learning_rate: 1.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 295ms/step - accuracy: 0.9688 - loss: 0.0648 - val_accuracy: 1.0000 - val_loss: 0.0506 - learning_rate: 1.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 3s/step - accuracy: 0.9664 - loss: 0.1067 - val_accuracy: 1.0000 - val_loss: 0.0268 - learning_rate: 1.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 269ms/step - accuracy: 0.9688 - loss: 0.0844 - val_accuracy: 0.9896 - val_loss: 0.0463 - learning_rate: 1.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 4s/step - accuracy: 0.9601 - loss: 0.0849 - val_accuracy: 1.0000 - val_loss: 0.0252 - learning_rate: 1.0000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 279ms/step - accuracy: 1.0000 - loss: 0.0322 - val_accuracy: 1.0000 - val_loss: 0.0332 - learning_rate: 1.0000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 4s/step - accuracy: 0.9723 - loss: 0.0874 - val_accuracy: 1.0000 - val_loss: 0.0129 - learning_rate: 1.0000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 331ms/step - accuracy: 0.9688 - loss: 0.0530 - val_accuracy: 0.9896 - val_loss: 0.0368 - learning_rate: 1.0000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3s/step - accuracy: 0.9869 - loss: 0.0606 - val_accuracy: 0.9792 - val_loss: 0.0425 - learning_rate: 1.0000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 257ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 0.0178 - learning_rate: 1.0000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9911 - loss: 0.0390\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 3s/step - accuracy: 0.9909 - loss: 0.0395 - val_accuracy: 1.0000 - val_loss: 0.0141 - learning_rate: 1.0000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 258ms/step - accuracy: 1.0000 - loss: 0.0188 - val_accuracy: 0.9792 - val_loss: 0.0293 - learning_rate: 5.0000e-05\n",
      "Epoch 29/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 3s/step - accuracy: 0.9877 - loss: 0.0375 - val_accuracy: 0.9896 - val_loss: 0.0262 - learning_rate: 5.0000e-05\n",
      "Epoch 30/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 243ms/step - accuracy: 1.0000 - loss: 0.0221 - val_accuracy: 1.0000 - val_loss: 0.0196 - learning_rate: 5.0000e-05\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "AmericanFootball       0.98      1.00      0.99        40\n",
      "      Basketball       1.00      1.00      1.00        40\n",
      "          Soccer       1.00      0.97      0.99        40\n",
      "          Tennis       1.00      1.00      1.00        40\n",
      "      Volleyball       1.00      1.00      1.00        40\n",
      "\n",
      "        accuracy                           0.99       200\n",
      "       macro avg       1.00      0.99      0.99       200\n",
      "    weighted avg       1.00      0.99      0.99       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 926ms/step\n",
      "\n",
      " Final-sub-saved!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load - analyze data\n",
    "    loader = DataLoader()\n",
    "    data = loader.load_data()\n",
    "    \n",
    "    # Split-data\n",
    "    X_train, X_test = data['X_train'], data['X_test']\n",
    "    y_train = data['y_train']\n",
    "    class_names = data['class_names']\n",
    "    \n",
    "    # Further-split-into-train-validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=Config.TEST_SIZE,\n",
    "        stratify=y_train,\n",
    "        random_state=Config.RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Initialize-&-trainmodel\n",
    "    model = NeuralNetworkModel(Config.NUM_CLASSES)\n",
    "    model.train(X_train, y_train)\n",
    "    \n",
    "    # Plot-training-history\n",
    "    model.plot_training_history()\n",
    "    \n",
    "    # Evaluate-model\n",
    "    model.evaluate(X_val, y_val, class_names)\n",
    "    \n",
    "    # model save\n",
    "    model.model.save('finalmodel.h5')\n",
    "    \n",
    "    # Make-predictions-on-test-set\n",
    "    test_preds = np.argmax(model.model.predict(X_test), axis=1)\n",
    "    test_labels = loader.le.inverse_transform(test_preds)\n",
    "    \n",
    "    # Create-submission-file\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": loader.test_df[\"id\"],\n",
    "        \"label\": test_labels\n",
    "    })\n",
    "    submission.to_csv(\"final_sub.csv\", index=False)\n",
    "    print(\"\\n Final-sub-saved!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shamila",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
