{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06616fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\shamila\\.conda\\envs\\shamila\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install tensorflow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e34ffc",
   "metadata": {},
   "source": [
    "# Import-Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "656eb6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, save_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a6524",
   "metadata": {},
   "source": [
    "# Config class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764003e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    BASE_PATH = \"C:/Users/Shamila/OneDrive/Desktop/CI_Assign\"\n",
    "    IMG_SIZE = 224\n",
    "    RANDOM_STATE = 42\n",
    "    TEST_SIZE = 0.2\n",
    "    VAL_SIZE = 0.15\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 30\n",
    "    NUM_CLASSES = None \n",
    "    \n",
    "    # Visualization settings\n",
    "    PLOT_STYLE = 'ggplot'\n",
    "    FIG_SIZE = (10, 6)\n",
    "    CMAP = 'YlGnBu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e3eb8",
   "metadata": {},
   "source": [
    "# Set-matplotlib-style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "285bc7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(Config.PLOT_STYLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba795e3a",
   "metadata": {},
   "source": [
    "# Data-Loader-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e08bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        self.train_df = pd.read_csv(os.path.join(Config.BASE_PATH, \"train.csv\"))\n",
    "        self.test_df = pd.read_csv(os.path.join(Config.BASE_PATH, \"test.csv\"))\n",
    "        self.le = LabelEncoder()\n",
    "        self.class_names = None\n",
    "\n",
    "    def _analyze_images(self, df):\n",
    "        \"dimensions and aspect ratios\"\n",
    "        widths = []\n",
    "        heights = []\n",
    "        aspect_ratios = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            img_path = os.path.join(Config.BASE_PATH, row['filename'])\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    width, height = img.size\n",
    "                    widths.append(width)\n",
    "                    heights.append(height)\n",
    "                    aspect_ratios.append(width / height)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        return widths, heights, aspect_ratios\n",
    "\n",
    "    def plot_image_stats(self):\n",
    "        \"statistics visualizations\"\n",
    "        widths, heights, aspect_ratios = self._analyze_images(self.train_df)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "        sns.histplot(widths, bins=30, color='mediumseagreen', ax=axes[0, 0], kde=True)\n",
    "        axes[0, 0].set_title('Image Width Distribution')\n",
    "        axes[0, 0].set_xlabel('Width (pixels)')\n",
    "        axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "        sns.histplot(heights, bins=30, color='seagreen', ax=axes[0, 1], kde=True)\n",
    "        axes[0, 1].set_title('Image Height Distribution')\n",
    "        axes[0, 1].set_xlabel('Height (pixels)')\n",
    "        axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "        sns.histplot(aspect_ratios, bins=30, color='palegreen', ax=axes[1, 0], kde=True)\n",
    "        axes[1, 0].set_title('Aspect Ratio Distribution')\n",
    "        axes[1, 0].set_xlabel('Aspect Ratio (width/height)')\n",
    "        axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "        sns.scatterplot(x=widths, y=heights, alpha=0.6, color='forestgreen', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Width vs Height Scatter Plot')\n",
    "        axes[1, 1].set_xlabel('Width (pixels)')\n",
    "        axes[1, 1].set_ylabel('Height (pixels)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('statistics.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Boxplot-aspect-ratios\n",
    "        plt.figure(figsize=Config.FIG_SIZE)\n",
    "        sns.boxplot(x=aspect_ratios, color='mediumseagreen')\n",
    "        plt.title('Aspect Ratio Boxplot')\n",
    "        plt.xlabel('Aspect Ratio')\n",
    "        plt.savefig('aspectratioboxplot.png')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_class_distribution(self):\n",
    "        \"Plot class distribution as pie chart and bar graph\"\n",
    "        class_counts = self.train_df['class'].value_counts()\n",
    "        self.class_names = class_counts.index.tolist()\n",
    "        Config.NUM_CLASSES = len(self.class_names)\n",
    "\n",
    "        # Pie-chart\n",
    "        plt.figure(figsize=Config.FIG_SIZE)\n",
    "        plt.pie(class_counts, labels=self.class_names, autopct='%1.1f%%',\n",
    "                startangle=90, colors=sns.color_palette('Greens', len(class_counts)))\n",
    "        plt.title('Class Distribution (Pie Chart)')\n",
    "        plt.savefig('piechart.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Bar-graph\n",
    "        plt.figure(figsize=Config.FIG_SIZE)\n",
    "        sns.barplot(x=class_counts.index, y=class_counts.values, palette='Greens')\n",
    "        plt.title('Class Distribution (Bar Graph)')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('distribution-bar.png')\n",
    "        plt.close()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"Load and preprocess images\"\n",
    "        def _process_df(df, label=True):\n",
    "            X, y = [], []\n",
    "            for _, row in df.iterrows():\n",
    "                img = cv2.imread(os.path.join(Config.BASE_PATH, row['filename']))\n",
    "                if img is None:\n",
    "                    continue\n",
    "\n",
    "                img_resized = cv2.resize(img, (Config.IMG_SIZE, Config.IMG_SIZE))\n",
    "                X.append(preprocess_input(img_resized))\n",
    "\n",
    "                if label:\n",
    "                    y.append(row['class'])\n",
    "\n",
    "            return (np.array(X), np.array(y)) if label else np.array(X)\n",
    "\n",
    "        # Plot-datastatistics\n",
    "        self.plot_image_stats()\n",
    "        self.plot_class_distribution()\n",
    "\n",
    "        # Load-trainingdata\n",
    "        X_train, y_train = _process_df(self.train_df, label=True)\n",
    "        X_test = _process_df(self.test_df, label=False)\n",
    "\n",
    "        # Encode-labels\n",
    "        y_encoded = self.le.fit_transform(y_train)\n",
    "\n",
    "        return {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_encoded,\n",
    "            'X_test': X_test,\n",
    "            'class_names': self.class_names\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7184eb",
   "metadata": {},
   "source": [
    "# Neural-Network-Model-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbbf2772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkModel:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.model = self.build_model()\n",
    "        self.history = None\n",
    "    \n",
    "    def build_model(self):\n",
    "        \" transfer learning used Build to EfficientNetB0 based model\"\n",
    "        base_model = EfficientNetB0(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(Config.IMG_SIZE, Config.IMG_SIZE, 3)\n",
    "        )\n",
    "        \n",
    "        #  Fine-tune-later-ones and Freeze-initial-layers\n",
    "        for layer in base_model.layers[:100]:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[100:]:\n",
    "            layer.trainable = True\n",
    "        \n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        outputs = Dense(self.num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = Model(inputs=base_model.input, outputs=outputs)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_data_augmenter(self):\n",
    "        \" augmentation used to create image data generator \"\n",
    "        return ImageDataGenerator(\n",
    "            rotation_range=30,\n",
    "            zoom_range=0.2,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            brightness_range=[0.8, 1.2],\n",
    "            shear_range=0.1,\n",
    "            fill_mode='nearest',\n",
    "            validation_split=Config.VAL_SIZE\n",
    "        )\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        \"data augmentation and callbacks used to train the model \"\n",
    "        datagen = self.get_data_augmenter()\n",
    "        \n",
    "        # Create - callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1),\n",
    "            ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            datagen.flow(X_train, y_train, batch_size=Config.BATCH_SIZE, subset='training'),\n",
    "            steps_per_epoch=int(len(X_train) * (1 - Config.VAL_SIZE) // Config.BATCH_SIZE),\n",
    "            \n",
    "            epochs=Config.EPOCHS,\n",
    "            validation_data=datagen.flow(X_train, y_train, batch_size=Config.BATCH_SIZE, subset='validation'),\n",
    "            validation_steps=int(len(X_train) * Config.VAL_SIZE // Config.BATCH_SIZE),\n",
    "            \n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "                )\n",
    "\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"Plot training & validation accuracy-loss\"\n",
    "        if self.history is None:\n",
    "            print(\"Model hasn't been trained yet!\")\n",
    "            return\n",
    "        \n",
    "        history = self.history.history\n",
    "        \n",
    "        plt.figure(figsize=(14, 5))\n",
    "        \n",
    "        # Plot-accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot-loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('traininghistory.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def evaluate(self, X_val, y_val, class_names):\n",
    "        \"Evaluate model & plot confusion matrix\"\n",
    "        y_pred = np.argmax(self.model.predict(X_val), axis=1)\n",
    "        \n",
    "        # Classification-report\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_val, y_pred, target_names=class_names))\n",
    "        \n",
    "        # Confusion-matrix\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=Config.CMAP, \n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusionmatrix.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b652f98",
   "metadata": {},
   "source": [
    "# Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ed6252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shamila\\AppData\\Local\\Temp\\ipykernel_9080\\1244372516.py:81: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=class_counts.index, y=class_counts.values, palette='Greens')\n",
      "c:\\Users\\Shamila\\.conda\\envs\\shamila\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.2489 - loss: 2.6319"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 7s/step - accuracy: 0.2523 - loss: 2.6131 - val_accuracy: 0.5625 - val_loss: 1.3632 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m 1/21\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 4s/step - accuracy: 0.5000 - loss: 1.6386"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shamila\\.conda\\envs\\shamila\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 614ms/step - accuracy: 0.5000 - loss: 1.6386 - val_accuracy: 0.6146 - val_loss: 1.3430 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.6194 - loss: 1.0615"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 4s/step - accuracy: 0.6217 - loss: 1.0561 - val_accuracy: 0.7917 - val_loss: 1.0102 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m 1/21\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 3s/step - accuracy: 0.6250 - loss: 0.9535"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 559ms/step - accuracy: 0.6250 - loss: 0.9535 - val_accuracy: 0.8021 - val_loss: 0.9676 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.7699 - loss: 0.5738"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 4s/step - accuracy: 0.7722 - loss: 0.5701 - val_accuracy: 0.8854 - val_loss: 0.6814 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m 1/21\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 3s/step - accuracy: 0.8750 - loss: 0.2571"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 526ms/step - accuracy: 0.8750 - loss: 0.2571 - val_accuracy: 0.8958 - val_loss: 0.6461 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8828 - loss: 0.3486"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 3s/step - accuracy: 0.8834 - loss: 0.3474 - val_accuracy: 0.9688 - val_loss: 0.3923 - learning_rate: 1.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 203ms/step - accuracy: 0.8750 - loss: 0.3585 - val_accuracy: 0.9688 - val_loss: 0.3960 - learning_rate: 1.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3s/step - accuracy: 0.9053 - loss: 0.2410 - val_accuracy: 0.9688 - val_loss: 0.2661 - learning_rate: 1.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 222ms/step - accuracy: 0.9688 - loss: 0.2053 - val_accuracy: 0.9688 - val_loss: 0.2684 - learning_rate: 1.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3s/step - accuracy: 0.9175 - loss: 0.2412 - val_accuracy: 0.9688 - val_loss: 0.1882 - learning_rate: 1.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 255ms/step - accuracy: 0.9062 - loss: 0.2224 - val_accuracy: 0.9688 - val_loss: 0.1844 - learning_rate: 1.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9418 - loss: 0.1467"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 4s/step - accuracy: 0.9418 - loss: 0.1467 - val_accuracy: 0.9896 - val_loss: 0.1137 - learning_rate: 1.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m 1/21\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 3s/step - accuracy: 0.9688 - loss: 0.0848"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 307ms/step - accuracy: 0.9688 - loss: 0.0848 - val_accuracy: 1.0000 - val_loss: 0.1096 - learning_rate: 1.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 4s/step - accuracy: 0.9608 - loss: 0.1310 - val_accuracy: 0.9583 - val_loss: 0.1095 - learning_rate: 1.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 301ms/step - accuracy: 0.9062 - loss: 0.2978 - val_accuracy: 0.9896 - val_loss: 0.0782 - learning_rate: 1.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 4s/step - accuracy: 0.9479 - loss: 0.1257 - val_accuracy: 1.0000 - val_loss: 0.0408 - learning_rate: 1.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 250ms/step - accuracy: 0.9688 - loss: 0.0868 - val_accuracy: 0.9896 - val_loss: 0.0556 - learning_rate: 1.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 4s/step - accuracy: 0.9720 - loss: 0.0671 - val_accuracy: 0.9792 - val_loss: 0.0564 - learning_rate: 1.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 246ms/step - accuracy: 1.0000 - loss: 0.1339 - val_accuracy: 0.9896 - val_loss: 0.0466 - learning_rate: 1.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 3s/step - accuracy: 0.9751 - loss: 0.0759 - val_accuracy: 1.0000 - val_loss: 0.0263 - learning_rate: 1.0000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 265ms/step - accuracy: 1.0000 - loss: 0.0158 - val_accuracy: 1.0000 - val_loss: 0.0098 - learning_rate: 1.0000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 3s/step - accuracy: 0.9821 - loss: 0.0584 - val_accuracy: 1.0000 - val_loss: 0.0134 - learning_rate: 1.0000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 288ms/step - accuracy: 1.0000 - loss: 0.0166 - val_accuracy: 0.9896 - val_loss: 0.0234 - learning_rate: 1.0000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 3s/step - accuracy: 0.9645 - loss: 0.0815 - val_accuracy: 1.0000 - val_loss: 0.0128 - learning_rate: 1.0000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m 1/21\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.9688 - loss: 0.0657\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 229ms/step - accuracy: 0.9688 - loss: 0.0657 - val_accuracy: 1.0000 - val_loss: 0.0102 - learning_rate: 1.0000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - accuracy: 0.9876 - loss: 0.0355 - val_accuracy: 1.0000 - val_loss: 0.0112 - learning_rate: 5.0000e-05\n",
      "Epoch 28/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 234ms/step - accuracy: 0.9688 - loss: 0.1918 - val_accuracy: 1.0000 - val_loss: 0.0046 - learning_rate: 5.0000e-05\n",
      "Epoch 29/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - accuracy: 0.9854 - loss: 0.0377 - val_accuracy: 0.9896 - val_loss: 0.0326 - learning_rate: 5.0000e-05\n",
      "Epoch 30/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 230ms/step - accuracy: 1.0000 - loss: 0.0075 - val_accuracy: 0.9896 - val_loss: 0.0424 - learning_rate: 5.0000e-05\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "AmericanFootball       0.98      1.00      0.99        40\n",
      "      Basketball       1.00      1.00      1.00        40\n",
      "          Soccer       1.00      0.97      0.99        40\n",
      "          Tennis       1.00      1.00      1.00        40\n",
      "      Volleyball       1.00      1.00      1.00        40\n",
      "\n",
      "        accuracy                           0.99       200\n",
      "       macro avg       1.00      0.99      0.99       200\n",
      "    weighted avg       1.00      0.99      0.99       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 943ms/step\n",
      "\n",
      " Final-gamemodel-saved!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load - analyze data\n",
    "    loader = DataLoader()\n",
    "    data = loader.load_data()\n",
    "    \n",
    "    # Split-data\n",
    "    X_train, X_test = data['X_train'], data['X_test']\n",
    "    y_train = data['y_train']\n",
    "    class_names = data['class_names']\n",
    "    \n",
    "    # Further-split-into-train-validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=Config.TEST_SIZE,\n",
    "        stratify=y_train,\n",
    "        random_state=Config.RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Initialize-&-trainmodel\n",
    "    model = NeuralNetworkModel(Config.NUM_CLASSES)\n",
    "    model.train(X_train, y_train)\n",
    "    \n",
    "    # Plot-training-history\n",
    "    model.plot_training_history()\n",
    "    \n",
    "    # Evaluate-model\n",
    "    model.evaluate(X_val, y_val, class_names)\n",
    "    \n",
    "    # model save\n",
    "    model.model.save('gamemodel.h5')\n",
    "    \n",
    "    # Make-predictions-on-test-set\n",
    "    test_preds = np.argmax(model.model.predict(X_test), axis=1)\n",
    "    test_labels = loader.le.inverse_transform(test_preds)\n",
    "    \n",
    "    # Create-submission-file\n",
    "    submission = pd.DataFrame({\n",
    "        \"id\": loader.test_df[\"id\"],\n",
    "        \"label\": test_labels\n",
    "    })\n",
    "    submission.to_csv(\"final_gamemodel.csv\", index=False)\n",
    "    print(\"\\n Final-gamemodel-saved!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shamila",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
